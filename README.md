# Qode Market Intelligence System

A production-ready data collection and analysis system for real-time Indian stock market intelligence through social media monitoring.

## ðŸŽ¯ Overview

This system scrapes Twitter/X for Indian stock market discussions, processes the data, and generates quantitative trading signals. Built with efficiency and scalability in mind.

## ðŸš€ Features

- **Selenium-based Scraper**: No API costs, bypasses rate limits
- **Real-time Processing**: Concurrent data collection and analysis
- **Smart Deduplication**: Hash-based efficient duplicate detection
- **Signal Generation**: ML-powered trading signal extraction
- **Memory Efficient**: Handles 10k+ tweets without breaking a sweat
- **Production Ready**: Proper logging, error handling, and monitoring

## ðŸ“‹ Requirements

- Python 3.14.2
- Chrome or Firefox browser
- 4GB RAM minimum
- Internet connection

## ðŸ”§ Installation

### Step 1: Clone Repository

```bash
git clone <your-repo-url>
cd qode-market-intel
```

### Step 2: Create Virtual Environment

```bash
python -m venv venv

# On Windows (Git Bash)
source venv/Scripts/activate

# On Linux/Mac
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### Step 4: Browser Setup

**For Chrome:**

- Make sure Chrome is installed
- ChromeDriver will be auto-installed by selenium-manager

**For Firefox:**

- Make sure Firefox is installed
- GeckoDriver will be auto-installed by selenium-manager

### Step 5: Configuration

```bash
cp .env.example .env
# Edit .env if needed (optional)
```

## ðŸƒ Usage

### Basic Run

```bash
python main.py
```

### Custom Configuration

Edit `config.yaml` to customize:

- Target tweet count
- Hashtags to monitor
- Scraping intervals
- Output formats

### Advanced Options

```bash
# Collect 5000 tweets
python main.py --target 5000

# Use specific browser
python main.py --browser firefox

# Custom time range
python main.py --hours 48
```

## ðŸ“Š Output

The system generates:

1. **Raw Data**: `data/tweets_YYYYMMDD_HHMMSS.parquet`
2. **Processed Data**: `output/processed_data.parquet`
3. **Trading Signals**: `output/signals.csv`
4. **Visualizations**: `output/dashboard.html`
5. **Logs**: `logs/market_intel_YYYYMMDD.log`

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Selenium  â”‚  â”€â”€>  Twitter/X Scraping
â”‚   Scraper   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data       â”‚  â”€â”€>  Cleaning, Dedup
â”‚  Processor  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Analyzer   â”‚  â”€â”€>  TF-IDF, Signals
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Storage    â”‚  â”€â”€>  Parquet Files
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/ -v

# Test scraper only
python -m pytest tests/test_scraper.py -v
```

## ðŸŽ¨ Code Structure

- `src/collectors/` - Web scraping logic
- `src/processors/` - Data cleaning and storage
- `src/analyzers/` - Signal generation and ML
- `src/visualizers/` - Plotting and dashboards
- `src/utils/` - Helper functions

## âš¡ Performance

- **Speed**: ~2000 tweets in 10-15 minutes
- **Memory**: <500MB for 5000 tweets
- **Storage**: ~1MB per 1000 tweets (compressed)

## ðŸ›¡ï¸ Anti-Detection

- Random user agents
- Human-like scroll patterns
- Variable delays
- Cookie management
- Headless mode with stealth settings

## ðŸ“ Technical Highlights

1. **Efficient Data Structures**: Using sets for O(1) dedup
2. **Concurrent Processing**: ThreadPoolExecutor for parallel tasks
3. **Memory Optimization**: Chunked processing, generators
4. **Error Recovery**: Automatic retry with exponential backoff

## ðŸ› Troubleshooting

**Chrome driver issues:**

```bash
# Update selenium
pip install --upgrade selenium
```

**Memory errors:**

```bash
# Reduce batch size in config.yaml
processing:
  batch_size: 100  # Lower this
```

**No tweets found:**

- Check internet connection
- Try different hashtags
- Increase scraping time

## ðŸ“ˆ Scaling

For 10x more data:

1. Enable distributed processing (uncomment in config.yaml)
2. Use cloud storage (S3/GCS integration ready)
3. Deploy on server with more RAM

## ðŸ¤ Contributing

This is a technical assignment, but improvements welcome:

1. Fork the repo
2. Create feature branch
3. Commit changes
4. Push and create PR

## ðŸ“„ License

MIT License - feel free to use for learning

## ðŸ‘¤ Author

Built for Qode Technical Assignment

## ðŸ™ Acknowledgments

- Built without any paid APIs
- Uses open-source tools only
- Respects Twitter's robots.txt
